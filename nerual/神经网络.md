
- 一般而言，“**朴素感知机**”是指**单层网络**，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用 sigmoid 函数等平滑的激活函数的多层网络。

- sigmoid函数的平滑性对神经网络的学习

- 使用线性函数的话，加深神经网络的层数就没有意义了。


1. 神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的 过程称为“学习”。神经网络的学习分成下面4个步骤。

 - 步骤1（mini-batch） 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们 的目标是减小mini-batch的损失函数的值。 
 - 步骤2（计算梯度） 为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。 
 - 步骤3（更新参数） 将权重参数沿梯度方向进行微小更新。、
 - 步骤4（重复） 重复步骤1、步骤2、步骤3。


2.  🧠 举个通俗的例子来理解：

	假设有60000张图片（训练样本），想训练10轮（epoch），每次拿100张训练：

| 名称             | 数值                       | 含义                           |
| -------------- | ------------------------ | ---------------------------- |
| batch_size     | 100                      | 每次训练看100张图片                  |
| iter_per_epoch | 600                      | 每训练600次就算一轮                  |
| iters_num      | 10000                    | 总共训练10000次，也就是 10000 个 batch |
| 总共训练轮数（epoch数） | 10000 / 600 = 16.66 ≈ 17 | 大概训练了 17 个 epoch             |


| 优化器                                  | 特点和适用场景                                                                                  |                       |
| ------------------------------------ | ---------------------------------------------------------------------------------------- | --------------------- |
| **SGD**（随机梯度下降）                      | 每次只用一个或一小批样本来更新参数，避免了全量梯度下降的计算开销。但容易在鞍点或局部最优处震荡，学习率要手动调节。                                | 最基础，适合教学和简化情况，收敛慢     |
| **Momentum**（动量法）                    | 在梯度方向上引入“惯性”，模拟小球下坡时加速的过程，公式中加入了之前梯度的指数加权平均，可缓解震荡并加快收敛。                                  | 更快收敛，适合深层网络           |
| **AdaGrad**                          | 根据每个参数的历史梯度平方累积，自适应调整学习率——更新频繁的参数学习率降低，适合处理稀疏数据。但长时间训练后学习率过低会停止学习。                       | 稀疏特征、多类别问题，如 NLP 早期模型 |
| **RMSProp**                          | 改进 AdaGrad，引入指数衰减的移动平均（滑动窗口）而不是简单累积，避免学习率快速消失，适合处理非平稳目标如 RNN。                            | 适合非凸目标函数（如 RNN）       |
| **Adam**（Adaptive Moment Estimation） | 结合 Momentum 和 RMSProp 的优点，使用一阶动量（梯度的滑动平均）和二阶动量（平方梯度的滑动平均），并引入偏差修正。具有良好的适应性，是当前最常用的优化器之一。 | ✅ 默认首选，适用于大多数神经网络训练任务 |

超参数的最优化的内容，简单归纳一如下所示。 
步骤0 设定超参数的范围。
步骤1 从设定的超参数范围中随机采样。
步骤2 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。
步骤3 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。


- 作为权重初始值，Xavier初始值、He初始值等比较有效
-  抑制过拟合的正则化技术有权值衰减、Dropout等。