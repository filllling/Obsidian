
# mlp神经网络
1. 输入层
     - 数据向量化
2. 隐藏层
    - sigmoid 
    - ReLU
3. 输出层
     - 恒等函数 （回归）
     - softmax  （多任务分类）


# 神经网络的学习

1. 损失函数
    - 均方误差
    - 交叉熵误差
    - 使用mini-batch进行学习
2. 数值微分
3. 随机梯度下降
4. 误差反向传播



# 学习技巧（调参）
1. 优化器的选择

| 优化器                                  | 特点和适用场景                                                                                  |                       |
| ------------------------------------ | ---------------------------------------------------------------------------------------- | --------------------- |
| **SGD**（随机梯度下降）                      | 每次只用一个或一小批样本来更新参数，避免了全量梯度下降的计算开销。但容易在鞍点或局部最优处震荡，学习率要手动调节。                                | 最基础，适合教学和简化情况，收敛慢     |
| **Momentum**（动量法）                    | 在梯度方向上引入“惯性”，模拟小球下坡时加速的过程，公式中加入了之前梯度的指数加权平均，可缓解震荡并加快收敛。                                  | 更快收敛，适合深层网络           |
| **AdaGrad**                          | 根据每个参数的历史梯度平方累积，自适应调整学习率——更新频繁的参数学习率降低，适合处理稀疏数据。但长时间训练后学习率过低会停止学习。                       | 稀疏特征、多类别问题，如 NLP 早期模型 |
| **RMSProp**                          | 改进 AdaGrad，引入指数衰减的移动平均（滑动窗口）而不是简单累积，避免学习率快速消失，适合处理非平稳目标如 RNN。                            | 适合非凸目标函数（如 RNN）       |
| **Adam**（Adaptive Moment Estimation） | 结合 Momentum 和 RMSProp 的优点，使用一阶动量（梯度的滑动平均）和二阶动量（平方梯度的滑动平均），并引入偏差修正。具有良好的适应性，是当前最常用的优化器之一。 | ✅ 默认首选，适用于大多数神经网络训练任务 |

2. 梯度消失与解决
    - Xavier初值：使用标准差为$\frac{1}{\sqrt{n}}$的高斯分布进行初始化 (使用sigmoid 和 tanh)
    - ”He初值“：使用标准差为$\sqrt{\frac{2}{n}}$的高斯分布
3. batch_norm
4. 过拟合
    - 正则化
    - dropout



