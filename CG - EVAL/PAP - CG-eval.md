# CG-Eval

## 1. **研究背景与目标**

本文针对中文大型语言模型（LLMs）的生成能力提出了首个全面评估框架**CG-Eval**，覆盖6大学科领域（科学与工程、人文社科、数学计算、医学考试、司法考试、注册会计师考试）和55个子学科[3][4]。旨在解决现有评估体系在文本生成质量度量上的不足，推动中文LLM发展[18]。

[**中文大模型评测基准概览表**](https://www.notion.so/1ddf3e75c00680b49ce6d44157ad6c2a?pvs=21)

---

## 2. **核心方法：CG-Eval基准**

- **评估维度**：
    - **问题类型**：术语定义、简答题、计算题
    - **评分系统**：
        - **非计算题**：结合BLEU-4、ROUGE-2、CHRF和文本嵌入余弦相似度的综合指标**Gscore**
        - **计算题**：结果准确性与解题过程质量双重评估（最终答案精度占70%，步骤相似度占30%）[9]
    - **自动化流程**：无需人工干预，使用预训练模型+滑动窗口编码处理长文本
- **数据规模**：包含11,000个问题，测试数据和提示模板公开于[Hugging Face](https://huggingface.co/datasets/Besteasy/CG-Eval)。

## **2.1 prompt:**

以下为{科目名称}科目的术语:{术语},请解释其含义,把回 复控制在{答案长度}个汉字左右。与“简答题”相关的提示格式如下:以 下是{科目名称}科目的问题,请解答并把回复控制在{答案长度}个汉字 左右。\n{问题}

以下是{subject}科目的问题,请进行计算并给出阿拉伯数字结果。 请直接返回数值结果,不需要任何的汉字解释。\n{题目} 以下是{科目名称}科目的问题,请 以“解:”开头给出解题过程,并在解题过程的最后换行,在最后一行以“ 最终答案:”开头,按顺序给出数值及其单位,采用英文逗号分割,例如 “最终答案:1元,1次,1公顷,1人”。\n{题目}

以下是初中、高中和大学数学的提示格式相同且尤为复杂。结构如下 :以下是{科目名称}科目的问题,请使用latex语法给出解题过程,并在 解题过程的最后换行,在最后一行以“最终答案:”开头,根据不同的题 目类型按照latex语法给出数值、表达式、导数、积分、方程的根。导数 根据题目表述采用latex语法按照y'或者f'(x)表示。如果方程的一个未知数 有多个解,答案采用形如“x=1或x=-3”的方式表示。如果方程有多个未知 数,答案采用形如“x=1,y=-3,z=5”的方式表示,用英文逗号分隔。以下为 需要解答的题目:\n{题目}

## 2.2Scoring System

### **BLEU（Papineni et al., 2002）**

- **优势**：
    1. 简单直观，易于理解；
    2. 自动评估，适用于大规模数据；
    3. 与人工评分高度相关；
    4. 可重复，结果稳定。
- **局限**：
    1. 只关注n-gram匹配，忽视语义；
    2. 对短句不友好；
    3. 不能识别多样化表达；
    4. 错误检测能力有限；
    5. 可能与用户实际满意度不符。

### **ROUGE（Lin et al., 2004）**

- **优势**：
    1. 平衡精确度与召回率；
    2. 与人工评估相关性高；
    3. 自动化，结果一致；
    4. 可扩展，可评估不同n-gram、顺序等。
- **局限**：
    1. 偏重召回，可能偏向冗长摘要；
    2. 同样局限于n-gram匹配；
    3. 依赖参考摘要的质量；
    4. 忽略冗余信息；
    5. 难以捕捉语义差异；
    6. 对无关信息可能过于宽容。

### **CHRF（Popović et al., 2015）**

- **优势**：
    
    1. 字符级评估，更适应复杂形态语言；
    2. 精细粒度，可识别细微差异；
    3. 容忍轻微拼写错误；
    4. 省去分词预处理；
    5. 与人工评价有较高相关性。
- **局限**：
    
    1. 计算成本高；
    2. 可能过度关注表面形式；
    3. 依赖参考答案的质量。
    
    ## **语义相似度（Semantic Similarity）**
    
    - **优势**：
        1. 提供丰富的语义表示；
        2. 泛化能力强；
        3. 省去手动特征工程；
        4. 上下文敏感，能区分语境。
    - **局限**：
        1. 计算与资源成本高；
        2. 难以解释高维向量；
        3. 可能忽略细节或语义偏差；
        4. 可能受训练数据偏差影响；
        5. 对异常情况敏感
    
    ## **Gscore：复合评估指标**
    
    - **设计理念**： 为克服单一指标局限，综合 BLEU4、ROUGE2、CHRF 和语义相似度，形成更全面、准确的评估。
        
    - **公式**（非数学类任务）：
        
        ```
        Gscore = 0.2 * BLEU4 + 0.25 * ROUGE2 + 0.25 * CHRF + 0.3 * 语义相似度
        ```
        
    - **语义相似度实现**： 使用中文预训练模型 `text2vec-large-chinese`，处理超长文本时采用滑动窗口编码方式取平均表示。
        

### **数学类任务的 Gscore 评估方法**

- **小学算术题**：
    - 完全正确得分1，否则为0；
    
    - Gscore = 所有题目的平均准确度。
    
- **文字题/中高等数学计算题**：
    - 提取最终答案 + 解题步骤；
        
    - 准确度 = 最终答案是否正确；
        
    - 步骤相似度 = CHRF分数（StepChrf）；
        
    - **Gscore 计算公式**：
        
        ```
        Gscore = Accuracy + (1 - Accuracy) * 0.3 * StepChrf
        ```
        

---

### 3. **关键实验结果**

- **整体表现**：
    - **GPT-4**以综合Gscore **41.12**位列第一，尤其在**数学计算**领域优势显著（得分46.15 vs 第二名22.69）。
    - **ChatGLM-Std**在医学执照考试(42.34)和司法考试(47.24)中领先；**Spark Desk**在注册会计师考试中最佳(43.77)。
- **细分领域亮点**：
    - **初等数学**：GPT-4得分71.69，远超其他模型。
    - **人文社科**：百川-13B-Chat以40.49分居首，部分模型因拒绝敏感问题得分偏低。
    - **数学模型**：小参数模型（如Ziya-LLaMA-13B-v1.1）在小学数学领域超越部分百亿级闭源模型[17]。

---

### 4. **贡献与局限**

- **创新点**：
    - 首个专注于中文LLM生成能力多领域自动化评估框架
    - 开源数据集和评分系统促进研究透明性
- **局限性**：
    - 未涵盖多轮对话、创意生成等复杂交互场景[18]
    - 评分系统对语义细微差异的捕捉能力待优化（如BLEU/ROUGE的词汇匹配局限性） 完整测试数据和结果可通过[评估网站](http://cgeval.besteasy.com/)访问[3][19]。 