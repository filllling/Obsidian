## 🔤 **什么是 n-gram？**

**n-gram** 是指从一段文本中，连续提取出长度为 n 的词或字符序列，是一种基础的语言建模方法。
### ✅ 示例（以词为单位）：
句子：**“我喜欢学习自然语言处理”**
- **1-gram（unigram）**:  
    `我`，`喜欢`，`学习`，`自然`，`语言`，`处理`
- **2-gram（bigram）**:  
    `我 喜欢`，`喜欢 学习`，`学习 自然`，`自然 语言`，`语言 处理`
- **3-gram（trigram）**:  
    `我 喜欢 学习`，`喜欢 学习 自然`，`学习 自然 语言`，`自然 语言 处理`
### 📌 作用：
- 捕捉词与词之间的上下文关系
- 广泛用于：语言模型、文本分类、信息检索等

## 🔁 **什么是 skip-gram？**

**skip-gram** 是一种将词嵌入向量的模型，由 **word2vec** 提出，是预测某个词的上下文的一种方式。
### ✅ 思路：
与 n-gram 不同，**skip-gram** 不是只考虑连续的词，而是给定一个中心词，预测它一定范围内的上下文词（可跳过部分词）。
### ✍️ 示例（窗口大小 = 2）：
句子：**“我 喜欢 学习 自然 语言 处理”**
- 中心词：`学习`
- 窗口内词：`我`，`喜欢`，`自然`，`语言`
- 训练样本对：(`学习`, `我`)、(`学习`, `喜欢`)、(`学习`, `自然`)、(`学习`, `语言`)
### 📌 特点：
- 不要求上下文连续，可以“跳跃”（skip）
- 适合训练词向量（word embeddings）
- 相较 n-gram 更能捕捉“稀疏”的上下文信息
## **✅ n-gram vs skip-gram 对比表：**

| 特征     | n-gram        | skip-gram    |
| :----- | :------------ | :----------- |
| 核心思想   | 提取连续的 n 个词    | 给定中心词预测上下文   |
| 是否连续   | 是             | 否（可以跳过词）     |
| 应用场景   | 文本特征提取、传统语言模型 | 训练词向量、神经语言模型 |
| 数据稀疏问题 | 较严重（n 越大越稀疏）  | 通过上下文窗口缓解稀疏  |
