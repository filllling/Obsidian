### 🧠 LSTM（Long Short-Term Memory）编码流程简介

LSTM 是一种**循环神经网络（RNN**结构，专门设计用于捕捉序列数据中的长期依赖信息。它通过引入门控机制（输入门、遗忘门、输出门）解决传统 RNN 中梯度消失的问题。

---
### 🏛 示例输入（法律类文本）：

`“原告与被告存在合同纠纷”`

---
### 🔧 编码流程：
1. **文本预处理：**
    - 分词：`[原告, 与, 被告, 存在, 合同, 纠纷]`
    - 映射为词向量（例如通过 Word2Vec / 预训练词嵌入）：
        `[  v_原告, v_与, v_被告, v_存在, v_合同, v_纠纷 ]`
        
2. **LSTM 编码：**
    
    - 每个词向量作为一个时间步输入给 LSTM。
    - LSTM 按顺序处理每一个词，更新内部状态：
        `h₁ = LSTM(v_原告)` 
        `h₂ = LSTM(v_与, h₁)` 
        `h₃ = LSTM(v_被告, h₂)` 
        `...` 
        `h₆ = LSTM(v_纠纷, h₅)`
    - 输出：可以选择最后一个 hidden state `h₆` 作为整体句子的编码表示。
        
3. **可选：双向 LSTM（BiLSTM）**

    - 增强上下文感知能力：

        `h_forward = LSTM(v_原告 → v_纠纷)` 
        `h_backward = LSTM(v_纠纷 → v_原告)`
        `h_final = [h_forward; h_backward]`

---

### 🧾 输出（编码结果）：

- 一个固定维度的向量，表示整句：

    `[0.15, -0.23, 0.67, ..., 0.02]  ← 可供下游任务使用（如分类、相似度匹配）`
---

### ✅ 优点：

- 擅长处理**中短文本序列**；
- 保持词语的顺序
- 对上下文具有一定建模能力。

### ⚠️ 局限：

- 编码效率低，尤其是长文本；
- 对远距离依赖建模能力仍有限（尽管比普通 RNN 好）；
- 无法并行处理，训练耗时。