## 🧠 什么是 Word Embeddings（词嵌入）？
**词嵌入** 是一种将“词语”转换成**向量表示**的技术，使得机器可以理解词语之间的“语义关系”。
### 🎯 核心思想：
将每个词映射为一个低维连续向量，使得语义相似的词在向量空间中更接近。

--- 
## 📦 为什么需要词嵌入？

传统方法（如 one-hot 编码）有两个问题：
- **维度高**：词汇量大时，每个词的向量非常稀疏。
- **无语义**：one-hot 编码无法表达“词之间的关系”。
例如：
- one-hot 编码中，`“猫”` 和 `“狗”` 距离和 `“猫”` 与 `“飞机”` 一样远。
- 而词嵌入能学习到 `“猫” ≈ “狗”`，`“飞机”` 距离它们较远。

---

## 🛠️ 常见的词嵌入方法

### 1. **Word2Vec**

- **Skip-gram**：给定中心词预测上下文
- **CBOW（Continuous Bag of Words）**：给定上下文预测中心词。

### 2. **GloVe（Global Vectors）**

- 利用词共现矩阵构建嵌入，兼顾全局统计和局部上下文。

### 3. **FastText**

- 在词的基础上加入字符 n-gram，提高对未登录词（OOV）的表示能力。

### 4. **Doc2Vec**

- Word2Vec 的“升级版”，用来获取**整段文本（句子、段落、文档**的向量表示。