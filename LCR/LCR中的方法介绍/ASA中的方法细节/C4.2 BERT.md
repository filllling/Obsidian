## 🔍 一、什么是 BERT 编码？

**BERT（Bidirectional Encoder Representations from Transformers）** 是一个基于 Transformer 编码器的语言模型，它可以对文本进行**深度的上下文理解**，并将整个句子或段落编码成一个向量或向量序列。

---

## 🧾 示例文本：

`原告与被告存在合同纠纷`

---

## 🧠 编码流程拆解：

### 🔹 1）分词与特殊标记

BERT 需要加上特殊标记 `[CLS]` 和 `[SEP]`，并进行分词（BERT 使用 WordPiece 分词）。
`原始文本 → [CLS], 原告, 与, 被告, 存在, 合, 同, 纠, 纷, [SEP]`
 ⚠️ 注意："合同" 可能会被拆成 "合" 和 "同"，这是 WordPiece 的处理方式。

---

### 🔹 2）嵌入层（Embedding）

每个分词会被转为向量（词嵌入），并加上：
- **位置嵌入**（Position Embedding）
- **段落嵌入**（Segment Embedding，如果是句子对任务）
这一层的输出是一个向量序列：
`[   v_[CLS], v_原告, v_与, ..., v_纠, v_##纷, v_[SEP] ]`

---

### 🔹 3）Transformer 编码层

将上述嵌入向量序列送入 **多层 Transformer Encoder**，每一层都包含：

- **多头自注意力机制（Multi-Head Attention）**
- **前馈神经网络（Feed-Forward NN）**
- **残差连接 + LayerNorm**
经过多层后，得到每个 token 的上下文表示。

---

### 🔹 4）获取最终句子表示

- **整句表示：取 `[CLS]` 位置的输出向量**
    `v_sentence = h_[CLS] ∈ ℝ^768（base模型）`
    这个向量被视为整个句子的编码结果，用于分类、相似度计算等任务。
- **逐词表示：取其他 token 的向量** 可用于命名实体识别（NER）、关键词提取等任务。

## 📌 特点总结：

| 特性      | 表现                   |
| ------- | -------------------- |
| 上下文建模方式 | 双向，强大的语义捕捉能力         |
| 编码粒度    | 句子级、词级都可以            |
| 表示能力    | 强，适用于分类、匹配、NER 等任务   |
| 适合应用    | 法律案情分析、案件相似度检索、情感分析等 |