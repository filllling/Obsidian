## 🧠 什么是 BERT Embedding？

**BERT Embedding** 是通过 BERT 模型提取的词或句子的上下文语义表示。  
它不仅仅是一个词的“静态向量”，而是**结合上下文动态生成的向量表示**。

---

## ⚙️ BERT 与传统词向量的区别

|特点|Word2Vec / GloVe|BERT Embedding|
|---|---|---|
|向量类型|静态（每个词一个固定向量）|动态（同一词在不同句中有不同向量）|
|上下文建模|只考虑邻近词（局部）|双向建模上下文（全局+双向）|
|表示粒度|词级|词级 + 句级 + 段落级|
|预训练方式|基于滑窗预测|掩蔽语言模型 + 下一句预测|

---

## ✅ 示例：动态词向量

举例说明：
句子1：**"He went to the bank to deposit money."**  
句子2：**"He sat by the river bank."**

传统 word2vec：
- `bank` 的向量相同（无法区分“银行”和“河岸”）

BERT：
- `bank` 在两个句子中会生成**不同的向量表示**，准确理解其上下文含义。

---

## 🛠 BERT Embedding 提取方式（简要流程）

1. **输入处理**：
    
    - 文本被分词（WordPiece 分词器）
    - 加入特殊符号 `[CLS]`（分类）和 `[SEP]`（分隔符）
        
2. **编码过程**：
    
    - 文本输入 BERT 编码器
    - 每个 token 生成一个维度为 768（base 模型）的向量
    
3. **获取嵌入**：
    
    - 可取 `[CLS]` 的向量作为句子表示
    - 也可以取某个词的 token 向量作为词表示
    - 或者对所有 token 向量平均/池化作为句子嵌入

---

## 📌 应用场景

| 场景     | 应用方式                       |
| ------ | -------------------------- |
| 文本分类   | 提取 `[CLS]` 向量 + 分类器        |
| 相似度计算  | 提取句向量，计算余弦相似度              |
| 命名实体识别 | 提取每个 token 的嵌入，送入序列标注模型    |
| 问答系统   | 使用 BERT 预测答案在段落中的位置        |
| 句子向量建模 | BERT + pooling（可选结合 SBERT） |

---

## 🔍 推荐工具：

- `transformers`（Hugging Face）库 → 直接使用 `bert-base-uncased` 等模型
- `bert-as-service`（已过时）或 `sentence-transformers`（推荐）