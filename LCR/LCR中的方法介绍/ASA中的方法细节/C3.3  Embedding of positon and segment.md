## 🧭 1. 什么是位置嵌入（Position Embedding）？

### ✅ 定义：

位置嵌入是一个向量，用来表示**词在句子中的位置**。  
因为 Transformer 本身不具有“顺序信息”（不像 LSTM 有天然的时间步），所以我们要**人为地告诉模型词序**。

### 🧠 示例：

假设我们有一个句子：
`[CLS] 原告 与 被告 存在 合同 纠纷 [SEP]`
每个词的位置是：
`0     1   2   3   4   5   6    7`
BERT 会为每个位置添加一个固定的向量，比如：

`pos_0 = [0.01, -0.02, ..., 0.05]  #对应[CLS]
`pos_1 = [0.03,  0.07, ..., 0.02]# 对应 原告 ...`

### ➕ 最后做什么？

词向量 + 位置向量 = 编码输入（保留语义 + 顺序信息）

---

## 📦 2. 什么是段落嵌入（Segment Embedding）？

### ✅ 定义：
段落嵌入（Segment Embedding）是告诉模型：**每个词属于哪一句话/哪一段**。  
这是为了解决句子对任务，比如判断两句话是否相关、哪句话是问句哪句是答案。
### 🧠 举例：两个句子的输入（句子对任务）

`[CLS] 原告与被告存在纠纷 [SEP] 合同已于去年终止 [SEP]`

- 第一句话（Segment A）：`[CLS], 原告, 与, 被告, 存在, 纠纷, [SEP]` → segment_id = 0
- 第二句话（Segment B）：`合同, 已, 于, 去年, 终止, [SEP]` → segment_id = 1

模型会为两段分别加上不同的嵌入向量，例如：
`segment_A = [0.1, 0.1, ..., 0.1] segment_B = [-0.1, -0.1, ..., -0.1]`

---

## 🔄 最终输入是三者之和：

每个 token 的最终输入向量是：
`Input = TokenEmbedding + PositionEmbedding + SegmentEmbedding`
这三部分共同编码了词的语义、顺序、所属段落，构成了 BERT 编码的输入。

---

📌 **总结对比：**

|类型|编码了什么信息|用途|
|---|---|---|
|词嵌入|当前词的语义|表达词的含义|
|位置嵌入|当前词在句中的顺序|加入位置信息，补偿Transformer的无序特性|
|段落嵌入|当前词属于哪一段/哪句话|区分句子对|