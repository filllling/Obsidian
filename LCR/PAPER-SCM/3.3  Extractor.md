## Bi-GRU

### **GRU基础**

- GRU是RNN的一种变体，通过引入**更新门**（update gate）和**重置门**（reset gate）来解决传统RNN的梯度消失问题。
- 更新门控制前一时刻信息保留多少到当前状态，重置门决定如何结合当前输入与历史信息。
- 相比LSTM，GRU结构更简单，参数更少，计算效率更高。

### **双向处理**

- Bi-GRU由两个GRU组成，一个**正向**（forward）处理序列从头到尾，一个**反向**（backward）处理序列从尾到头。
- 最终输出结合正向和反向的隐藏状态，捕捉序列的上下文信息（过去和未来信息），适合需要全局上下文的任务，如文本分类、机器翻译等。

### 实现思路

- **输入**：句子以词嵌入（如预训练的GloVe或BERT嵌入）形式输入，每个词为固定维度的向量。
- **Bi-GRU**：使用双向GRU处理句子序列，提取正向和反向的隐藏状态。
- **语义表示**：将Bi-GRU的最终隐藏状态拼接或池化（如取平均或最大值）作为句子的语义向量。
- **输出**：句子的语义向量（固定维度）。

## Bi-CNN

### CNN基础

- CNN通过卷积核捕获序列中的局部特征（如n-gram），对文本中的短语或模式敏感，计算效率高。

### 实现思路

- **输入**：句子以词嵌入形式输入（每个词为固定维度向量）。
- **Bi-CNN**：
    - 正向CNN：使用多个卷积核（不同窗口大小）提取前向局部特征。
    - 反向CNN：对序列反转后进行卷积，提取后向局部特征。
    - 池化：对正向和反向卷积输出进行最大池化或平均池化，提取关键特征。
- **输出**：拼接正向和反向特征，生成句子的语义向量。

## open-clap-based on all civil documents

清华大学自然语言处理与社会人文计算研究中心（THUNLP）发布的一个开源中文预训练语言模型集合。该项目旨在提供多领域的中文预训练模型，特别适用于法律和百科等专业文本处理。